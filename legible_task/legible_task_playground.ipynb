{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from src.mazeworld import SimpleWallMazeWorld2\n",
    "from src.mdp import MDP, LegibleTaskMDP, Utilities\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "np.set_printoptions(precision=9, linewidth=2000, threshold=10000, suppress=True)\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically gets list of all possible goal states\n",
    "#   states: world states\n",
    "#   goal: MDP goal, can be a single goal like \"A\" or a composition of goals \"ABC\"\n",
    "#   with_objs: defines if the states include the existence of an objective in the state or not\n",
    "#   goals: used only if with_objs is false, list of tuples with the goals and the corresponding goals\n",
    "def get_goal_states(states: np.ndarray, goal: str, with_objs: bool=True, goals: List[Tuple]=[]):\n",
    "\n",
    "    if with_objs:\n",
    "        state_lst = list(states)\n",
    "        return [state_lst.index(x) for x in states if x.find(goal) != -1]\n",
    "    else:\n",
    "        for g in goals:\n",
    "            if g[2].find(goal) != -1:\n",
    "                return str(g[0]) + ' ' + str(g[1])\n",
    "\n",
    "# Gets a list of all possible initial states\n",
    "#   states: world states\n",
    "#   task_locs: list of tuples with the states of each possible goal in the scenario\n",
    "def get_initial_states(states: np.ndarray, task_locs: List[Tuple]):\n",
    "\n",
    "    return [state for state in states if state.find('N') != -1] + \\\n",
    "           [str(e[0]) + ' ' + str(e[1]) + ' ' + e[2] for e in task_locs]\n",
    "\n",
    "# Simulates the performance of an optimal MDP vs a legible MDP\n",
    "#   mdp: optimal MDP\n",
    "#   pol: optimal policy\n",
    "#   leg_mdp: legible MDP\n",
    "#   leg_pol: legible policy\n",
    "#   x0: initial position for the trajectories\n",
    "#   n_trajs: number of trajectories to test performance\n",
    "#   rng_gen: Random number generator\n",
    "def simulate(mdp: MDP, pol: np.ndarray, leg_mdp: LegibleTaskMDP, leg_pol: np.ndarray, x0: str,\n",
    "             n_trajs: int, goal: int, rng_gen: np.random.Generator):\n",
    "\n",
    "    mdp_trajs = []\n",
    "    tasks_trajs = []\n",
    "\n",
    "    for _ in tqdm(range(n_trajs), desc='Simulate Trajectories'):\n",
    "        traj, acts = mdp.trajectory(x0, pol, rng_gen)\n",
    "        traj_leg, acts_leg = leg_mdp.trajectory(x0, leg_pol, rng_gen)\n",
    "        mdp_trajs += [[traj, acts]]\n",
    "        tasks_trajs += [[traj_leg, acts_leg]]\n",
    "\n",
    "    mdp_r = mdp.trajectory_reward(mdp_trajs)\n",
    "    mdp_rl = leg_mdp.trajectory_reward(mdp_trajs, goal)\n",
    "    task_r = mdp.trajectory_reward(tasks_trajs)\n",
    "    task_rl = leg_mdp.trajectory_reward(tasks_trajs, goal)\n",
    "\n",
    "    return mdp_r, mdp_rl, task_r, task_rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure for the given scenario\n",
    "#   n_rows: number of rows in the maze world\n",
    "#   n_cols: number of columns in the maze world\n",
    "#   obj_place: list of tuples with the position for each goal and respective goal\n",
    "#   walls: list of walls that make the maze world\n",
    "def create_world_view(n_rows: int, n_cols: int, obj_place: List, walls: List=None):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot()\n",
    "    plt.xlim(0, n_cols)\n",
    "    plt.ylim(0,  n_rows)\n",
    "    plt.xticks([i + 1 for i in range(n_cols)])\n",
    "    plt.yticks([i + 1 for i in range(n_rows)])\n",
    "    ax.tick_params(axis='x', which='both', bottom=False, top=False)\n",
    "    ax.tick_params(axis='y', which='both', left=False, right=False)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.xaxis.set_minor_locator(ticker.FixedLocator([x + 0.5 for x in range(n_cols + 1)]))\n",
    "    ax.xaxis.set_minor_formatter(ticker.FixedFormatter([str(x) for x in range(1, n_cols + 1)]))\n",
    "    ax.set_yticklabels('')\n",
    "    ax.yaxis.set_minor_locator(ticker.FixedLocator([x + 0.5 for x in range(n_rows + 1)]))\n",
    "    ax.yaxis.set_minor_formatter(ticker.FixedFormatter([str(x) for x in range(1, n_rows + 1)]))\n",
    "    plt.grid(True)\n",
    "\n",
    "    for obj in obj_place:\n",
    "        # position of each goal\n",
    "        x = obj[1] - 0.5\n",
    "        y = obj[0] - 0.5\n",
    "        o = '$' + obj[2] + '$'\n",
    "        \n",
    "        plt.plot(x, y, marker=o, color='k', markersize=10)\n",
    "    \n",
    "    if walls:\n",
    "        for wall in walls:\n",
    "            wall_sec = np.array([list(wall_elem) for wall_elem in wall])\n",
    "            plt.plot(wall_sec[:, 1] - 0.5, wall_sec[:, 0] - 0.5, color='k', linewidth=5)\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "# Place a given trajectory in a figure of the maze world being used\n",
    "#   x0: tuple with trajectory's initial position in the maze world\n",
    "#   trajectory: sequence of positions in the maze and actions to draw\n",
    "#   figure: figure of the maze world to be used\n",
    "#   color: color to draw the trajectory\n",
    "#   goal: tuple with trajectory's final position in the maze world\n",
    "#   zorder: integer that gives the vertical position of the trajectory, useful when \n",
    "#           reusing the same figure to draw multiple trajectories\n",
    "def visualize_trajectory(x0: Tuple, trajectory: np.ndarray, figure: plt.Figure, color: str, goal: Tuple, zorder: int):\n",
    "    \n",
    "    actions = {'U': (0, 0.8), 'D': (0, -0.8), 'L':(-0.8, 0), 'R':(0.8, 0), 'G': (0, 0), 'P':(0, 0), 'N': (0, 0)}\n",
    "    ax = figure[1]\n",
    "    \n",
    "    plt.plot(x0[0]-0.5, x0[1]-0.5, marker='o', markersize=15, color='dimgrey', zorder=0)\n",
    "    plt.plot(goal[1]-0.5, goal[0]-0.5, marker='x', markersize=20, color='gold')\n",
    "    for ptr in trajectory:\n",
    "        x = ptr[0]-0.5\n",
    "        y = ptr[1]-0.5\n",
    "        a = actions[ptr[2]]\n",
    "        if ptr[2] in ['U', 'D', 'L', 'R']:\n",
    "            ax.arrow(x, y, a[0], a[1], head_width=0.1, head_length=0.1, lw=1.5, fc=color, ec=color, zorder=zorder)\n",
    "        else:\n",
    "            ax.add_patch(plt.Circle((x, y), 0.3, linewidth=1.7, fill=False, color=color))\n",
    "\n",
    "# Draw all possible actions a policy might choose for each state\n",
    "#   figure: figure of the maze world to be used\n",
    "#   pol: policy to draw\n",
    "#   goal_states: list of the possible goal states\n",
    "#   states: world states\n",
    "#   action_lst: list of the different world actions\n",
    "#   color: color to draw the policy\n",
    "#   objs: defines if the state definition includes possible environment objects and goals\n",
    "def draw_policy_states(figure: plt.Figure, pol: np.ndarray, goal_states: List, states: np.ndarray, \n",
    "                       action_lst: List, color: str, objs: bool=True):\n",
    "    \n",
    "    actions = {'U': (0, 0.8), 'D': (0, -0.8), 'L':(-0.8, 0), 'R':(0.8, 0), 'G': (0, 0), 'P':(0, 0), 'N': (0, 0)}\n",
    "    state_lst = list(states)\n",
    "    ax = figure[1]\n",
    "    \n",
    "    for state in states:\n",
    "        state_idx = state_lst.index(state)\n",
    "        if objs:\n",
    "            state_split = re.match(r\"([0-9]+) ([0-9]+) ([a-z]+)\", state, re.I)\n",
    "        else:\n",
    "            state_split = re.match(r\"([0-9]+) ([0-9]+)\", state, re.I)\n",
    "        y = int(state_split.group(1)) - 0.5\n",
    "        x = int(state_split.group(2)) - 0.5\n",
    "        if state_lst.index(state) in goal_states:\n",
    "            ax.add_patch(plt.Circle((x, y), 0.3, linewidth=1.7, color='gold', zorder=0))\n",
    "        pol_actions = np.nonzero(pol[state_idx, :])[0]\n",
    "        for action in pol_actions:\n",
    "            act = action_lst[action]\n",
    "            a = actions[act]\n",
    "            if act in ['U', 'D', 'L', 'R']:\n",
    "                ax.arrow(x, y, a[0], a[1], head_width=0.1, head_length=0.1, lw=1.5, fc=color, ec=color)\n",
    "            else:\n",
    "                ax.add_patch(plt.Circle((x, y), 0.3, linewidth=1.7, fill=False, color=color))\n",
    "\n",
    "# Combine a state trajectory and an action trajectory to one single sequence of state-actions\n",
    "#    trajectory: sequence of states\n",
    "#    actions: sequence of actions\n",
    "#    objs: defines if the state definition includes possible environment objects and goals\n",
    "def process_trajectory(trajectory: np.ndarray, actions: np.ndarray, objs: bool=True):\n",
    "    \n",
    "    traj = []\n",
    "    \n",
    "    for i in range(len(trajectory) - 1):\n",
    "        if objs:\n",
    "            state_split = re.match(r\"([0-9]+) ([0-9]+) ([a-z]+)\", trajectory[i], re.I)\n",
    "        else:\n",
    "            state_split = re.match(r\"([0-9]+) ([0-9]+)\", trajectory[i], re.I)\n",
    "        y = int(state_split.group(1))\n",
    "        x = int(state_split.group(2))\n",
    "        \n",
    "        traj += [(x, y, actions[i])]\n",
    "    \n",
    "    return traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Maze world dimensions\n",
    "n_rows = 10\n",
    "n_cols = 10\n",
    "\n",
    "# Maze world position of each possible goal\n",
    "objs_states = [(9, 3, 'P'), (9, 1, 'D'), (1, 6, 'C'), (5, 4, 'L'), (10, 8, 'T'), (7, 9, 'O')]\n",
    "\n",
    "# Maze world walls\n",
    "walls = [\n",
    "         # walls surrounding the world\n",
    "         [(0.5, x + 0.5) for x in range(0, n_cols + 1)],\n",
    "         [(n_rows + 0.5, x + 0.5) for x in range(0, n_cols + 1)], \n",
    "         [(x + 0.5, 0.5) for x in range(0, n_rows + 1)], \n",
    "         [(x + 0.5, n_cols + 0.5) for x in range(0, n_rows + 1)],\n",
    "         \n",
    "         # horizontal walls\n",
    "         [(x + 0.5, 2.5) for x in range(2, 6)],\n",
    "         [(x + 0.5, 2.5) for x in range(6, 10)],\n",
    "         [(x + 0.5, 6.5) for x in range(0, 3)],\n",
    "         [(x + 0.5, 8.5) for x in range(3, 8)],\n",
    "         [(x + 0.5, 8.5) for x in range(8, 11)],\n",
    "         \n",
    "         # vertical walls\n",
    "         [(0.5, 2.5), (1.5, 2.5)],\n",
    "         [(3.5, 6.5), (3.5, 7.5)],\n",
    "         [(3.5, 0.5), (3.5, 1.5)],\n",
    "         [(3.5, x + 0.5) for x in range(2, 6)],\n",
    "         [(3.5, x + 0.5) for x in range(8, 10)],\n",
    "         [(8.5, x + 0.5) for x in range(2, 4)],\n",
    "         [(8.5, x + 0.5) for x in range(4, 8)]]\n",
    "\n",
    "# Default initial state\n",
    "x0 = '1 1 N'\n",
    "\n",
    "# List of world possible goals and default goal\n",
    "goals = ['P', 'D', 'C', 'L', 'T', 'O']\n",
    "goal = 'D'\n",
    "\n",
    "# Visualize world\n",
    "fig, _ = create_world_view(n_rows, n_cols, objs_states, walls)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maze world dimensions\n",
    "n_rows = 10\n",
    "n_cols = 10\n",
    "\n",
    "# Maze world position of each possible goal\n",
    "objs_states = [(1, 7, 'P'), (1, 10, 'D'), (10, 10, 'C'), (10, 2, 'L'), (10, 6, 'T'), (7, 1, 'O')]\n",
    "\n",
    "# Maze world walls\n",
    "walls = [\n",
    "         # walls surrounding the world\n",
    "         [(0.5, x + 0.5) for x in range(0, n_cols + 1)],\n",
    "         [(n_rows + 0.5, x + 0.5) for x in range(0, n_cols + 1)], \n",
    "         [(x + 0.5, 0.5) for x in range(0, n_rows + 1)], \n",
    "         [(x + 0.5, n_cols + 0.5) for x in range(0, n_rows + 1)],\n",
    "         \n",
    "         # horizontal walls\n",
    "         [(0.5, 3.5), (1.5, 3.5)],\n",
    "         [(x + 0.5, 3.5) for x in range(3, 6)],\n",
    "         [(x + 0.5, 3.5) for x in range(6, 9)],\n",
    "         [(x + 0.5, 4.5) for x in range(0, 4)],\n",
    "         [(x + 0.5, 4.5) for x in range(9, 11)],\n",
    "         [(x + 0.5, 7.5) for x in range(9, 11)],\n",
    "         [(x + 0.5, 8.5) for x in range(0, 4)],\n",
    "         [(x + 0.5, 8.5) for x in range(7, 10)],\n",
    "         [(x + 0.5, 8.5) for x in range(5, 7)],\n",
    "         \n",
    "         # vertical walls\n",
    "         [(2.5, x + 0.5) for x in range(0, 4)],\n",
    "         [(2.5, x + 0.5) for x in range(9, 11)],\n",
    "         [(8.5, x + 0.5) for x in range(0, 2)],\n",
    "         [(8.5, x + 0.5) for x in range(2, 4)],\n",
    "         [(8.5, x + 0.5) for x in range(4, 6)],\n",
    "         [(8.5, x + 0.5) for x in range(6, 8)],\n",
    "         [(5.5, x + 0.5) for x in range(8, 11)],\n",
    "         [(3.5, x + 0.5) for x in range(0, 2)],\n",
    "         [(3.5, x + 0.5) for x in range(2, 4)],\n",
    "         [(3.5, x + 0.5) for x in range(4, 6)],\n",
    "         [(3.5, x + 0.5) for x in range(6, 8)]\n",
    "        ]\n",
    "\n",
    "# Default initial state\n",
    "x0 = '1 1 N'\n",
    "\n",
    "# List of world possible goals and default goal\n",
    "goals = ['P', 'D', 'C', 'L', 'T', 'O']\n",
    "goal = 'T'\n",
    "\n",
    "# Visualize world\n",
    "fig, _ = create_world_view(n_rows, n_cols, objs_states, walls)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 10\n",
    "n_cols = 10\n",
    "objs_states = [(1, 7, 'P'), (10, 10, 'D'), (7, 10, 'C'), (9, 1, 'L'), (9, 5, 'T'), (5, 1, 'O')]\n",
    "walls = [[(0.5, x + 0.5) for x in range(0, n_cols + 1)],\n",
    "         [(n_rows + 0.5, x + 0.5) for x in range(0, n_cols + 1)], \n",
    "         [(x + 0.5, 0.5) for x in range(0, n_rows + 1)], \n",
    "         [(x + 0.5, n_cols + 0.5) for x in range(0, n_rows + 1)],\n",
    "         \n",
    "         [(x + 0.5, 6.5) for x in range(4, 7)], \n",
    "         [(x + 0.5, 4.5) for x in range(4, 7)],\n",
    "         [(x + 0.5, 2.5) for x in range(3, 6)],\n",
    "         [(x + 0.5, 2.5) for x in range(6, 8)],\n",
    "         [(x + 0.5, 1.5) for x in range(8, 10)],\n",
    "         [(x + 0.5, 3.5) for x in range(0, 2)],\n",
    "         [(x + 0.5, 3.5) for x in range(8, 11)],\n",
    "         [(x + 0.5, 4.5) for x in range(0, 3)],\n",
    "         [(x + 0.5, 7.5) for x in range(0, 3)],\n",
    "         [(x + 0.5, 7.5) for x in range(8, 11)],\n",
    "         [(x + 0.5, 8.5) for x in range(0, 4)],\n",
    "         [(x + 0.5, 8.5) for x in range(4, 7)],\n",
    "         [(x + 0.5, 8.5) for x in range(8, 10)],\n",
    "         \n",
    "         [(2.5, x + 0.5) for x in range(0, 3)],\n",
    "         [(2.5, x + 0.5) for x in range(4, 6)],\n",
    "         [(2.5, x + 0.5) for x in range(6, 8)],\n",
    "         [(3.5, x + 0.5) for x in range(0, 3)],\n",
    "         [(3.5, x + 0.5) for x in range(9, 11)],\n",
    "         [(4.5, x + 0.5) for x in range(9, 11)],\n",
    "         [(4.5, x + 0.5) for x in range(4, 7)],\n",
    "         [(6.5, x + 0.5) for x in range(4, 7)],\n",
    "         [(7.5, x + 0.5) for x in range(0, 3)],\n",
    "         [(7.5, x + 0.5) for x in range(8, 11)],\n",
    "         [(8.5, x + 0.5) for x in range(0, 2)],\n",
    "         [(8.5, x + 0.5) for x in range(3, 6)],\n",
    "         [(8.5, x + 0.5) for x in range(6, 8)],\n",
    "         [(8.5, x + 0.5) for x in range(8, 10)]\n",
    "        ]\n",
    "x0 = '1 1 N'\n",
    "goals = ['P', 'D', 'C', 'L', 'T', 'O']\n",
    "goal = 'C'\n",
    "\n",
    "fig, _ = create_world_view(n_rows, n_cols, objs_states, walls)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 10\n",
    "n_cols = 10\n",
    "objs_states = [(8, 3, 'P'), (5, 7, 'D'), (5, 2, 'C'), (8, 7, 'T'), (1, 4, 'L'), (1, 7, 'O')]\n",
    "walls = [[(0.5, x + 0.5) for x in range(0, n_cols + 1)],\n",
    "         [(n_rows + 0.5, x + 0.5) for x in range(0, n_cols + 1)], \n",
    "         [(x + 0.5, 0.5) for x in range(0, n_rows + 1)], \n",
    "         [(x + 0.5, n_cols + 0.5) for x in range(0, n_rows + 1)], \n",
    "         \n",
    "         [(x + 0.5, 1.5) for x in range(2, 7)],\n",
    "         [(x + 0.5, 1.5) for x in range(7, 10)],\n",
    "         [(x + 0.5, 3.5) for x in range(0, 3)],\n",
    "         [(x + 0.5, 3.5) for x in range(3, 7)],\n",
    "         [(x + 0.5, 3.5) for x in range(7, 9)],\n",
    "         [(x + 0.5, 5.5) for x in range(0, 3)],\n",
    "         [(x + 0.5, 5.5) for x in range(7, 9)],\n",
    "         [(x + 0.5, 7.5) for x in range(0, 3)],\n",
    "         [(x + 0.5, 6.5) for x in range(4, 7)],\n",
    "         [(x + 0.5, 8.5) for x in range(4, 6)],\n",
    "         [(x + 0.5, 8.5) for x in range(7, 9)],\n",
    "         [(x + 0.5, 9.5) for x in range(2, 7)],\n",
    "         [(x + 0.5, 9.5) for x in range(7, 10)],\n",
    "         \n",
    "         [(2.5, x + 0.5) for x in range(1, 3)],\n",
    "         [(2.5, x + 0.5) for x in range(4, 7)],\n",
    "         [(3.5, x + 0.5) for x in range(2, 4)],\n",
    "         [(4.5, x + 0.5) for x in range(6, 8)],\n",
    "         [(6.5, x + 0.5) for x in range(2, 4)],\n",
    "         [(6.5, x + 0.5) for x in range(6, 9)],\n",
    "         [(7.5, x + 0.5) for x in range(1, 4)],\n",
    "         [(7.5, x + 0.5) for x in range(5, 9)],\n",
    "         [(9.5, x + 0.5) for x in range(1, 5)],\n",
    "         [(9.5, x + 0.5) for x in range(5, 10)],\n",
    "        ]\n",
    "x0 = '1 1 N'\n",
    "goals = ['P', 'D', 'C', 'T', 'L', 'O']\n",
    "goal = 'T'\n",
    "max_goal_len = max([len(g) for g in goals]) + 2\n",
    "\n",
    "fig, _ = create_world_view(n_rows, n_cols, objs_states, walls)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('##########################################')\n",
    "print('#####  Wall Auto Collect Maze World  #####')\n",
    "print('##########################################')\n",
    "beta = 0.5                      # used in the LegibleTaskMDP to define how close to the optimal to follow\n",
    "\n",
    "wacmw = SimpleWallMazeWorld2()  # type of world to use, this is a simplified version of the maze world, \n",
    "                                # where each goal is just to \"visit\" one location instead of a sequence of locations\n",
    "                                # and the robot just needs to pass by the local to consider it \"visited\"\n",
    "\n",
    "X_w, A_w, P_w = wacmw.generate_world(n_rows, n_cols, objs_states, walls, 'stochastic', 0.15) # use the world definitions before to create the world\n",
    "\n",
    "with_objs = True                # world states include if the state has a possible goal or if it is neutral \"N\"\n",
    "\n",
    "goal_states = get_goal_states(X_w, goal) # possible goal state indexes\n",
    "\n",
    "print('### Computing Costs and Creating Task MDPs ###')\n",
    "legible_function = 'leg_optimal' # there are two legible functions possible: 'leg_optimal' and 'leg_weight'\n",
    "                                 # 'leg_optimal' is the more simple and default legible function, 'leg_weight'\n",
    "                                 # is more complex but in most cases not needed\n",
    "opt_mdps = {}                    # dictionary that stores the optimal MDPs for each goal\n",
    "opt_v_mdps = {}                  # dictionary that stores the expected optimal reward for each state (used by the 'leg_weight' function)\n",
    "opt_q_mdps = {}                  # dictionary that stores the expected optimal reward for each state-action pair\n",
    "leg_mdps = {}                    # dictionary that stores the legible MDPs for each goal\n",
    "dists = []                       # list of average distance between each state and each goal state (used by the 'leg_weight' function)\n",
    "rng_gen = np.random.default_rng(int(time.time())) # random number generator used for randomization calls\n",
    "verbosity = False                # defines if we want extra info from the MDPs when computing the final policies\n",
    "\n",
    "print('### Defining the MDPs for each different goal ###')\n",
    "for i in range(len(goals)):\n",
    "    # Generate rewards for each goal and corresponding MDP\n",
    "    c = wacmw.generate_rewards(goals[i], X_w, A_w)\n",
    "    # MDP\n",
    "    # LegibleTaskMDP\n",
    "    #    x - world states\n",
    "    #    a - world actions\n",
    "    #    p - world state transition probabilities\n",
    "    #    c - MDP reward function\n",
    "    #    gamma - discount factor\n",
    "    #    goal_states - list of the MDP's goal states\n",
    "    #    feedback_type - 'costs' or 'rewards' depending on if we are using costs or rewards as the reward function\n",
    "    #    verbose - defines if we want extra information from internal methods\n",
    "    mdp = MDP(X_w, A_w, P_w, c, 0.9, get_goal_states(X_w, goals[i]), 'rewards', verbosity)\n",
    "    \n",
    "    # Find optimal policy, optimal q-values and optimal expected state rewards\n",
    "    pol, q = mdp.policy_iteration()\n",
    "    v = Utilities.v_from_q(q, pol)\n",
    "    \n",
    "    # Store q-values, expected rewards and optimal MDP\n",
    "    opt_q_mdps[goals[i]] = q\n",
    "    opt_v_mdps[goals[i]] = v\n",
    "    #dists += [mdp.policy_dist(pol, rng_gen)] # uncomment this line if you use the 'leg_weight' legible function\n",
    "    opt_mdps['mdp' + str(i + 1)] = mdp\n",
    "dists = np.array(dists)\n",
    "\n",
    "print('### Defining Legible MDP for intended goal ###')\n",
    "# LegibleTaskMDP\n",
    "#    x - world states\n",
    "#    a - world actions\n",
    "#    p - world state transition probabilities\n",
    "#    gamma - discount factor\n",
    "#    verbose - defines if we want extra information from internal methods\n",
    "#    task - legible MDP's goal\n",
    "#    task_states - list of each goal's maze world position (x, y, goal)\n",
    "#    tasks - list of possible goals\n",
    "#    beta - constant that define how close the legible policy follows the optimal policy \n",
    "#    goal_states - list of the legible MDP's goal states\n",
    "#    sign - 1 or -1 whether we are using rewards or costs as the optimal MDP's reward function\n",
    "#    leg_func - legible function being used\n",
    "#    q_mdps - optimal q-values for each goal\n",
    "#    v_mdps - optimal expected state rewards for each goal (used only by 'legible_weight' function)\n",
    "#    dists - average distance between each state and each goal (used only by 'legible_weight' function)\n",
    "leg_mdp = LegibleTaskMDP(X_w, A_w, P_w, 0.9, verbosity, goal, objs_states, goals, beta, goal_states, 1, \n",
    "                         legible_function, q_mdps=opt_q_mdps, v_mdps=opt_v_mdps, dists=dists)\n",
    "\n",
    "print('### Computing Optimal policy ###')\n",
    "time1 = time.time()\n",
    "opt_pol, opt_q = opt_mdps['mdp' + str(goals.index(goal) + 1)].policy_iteration()\n",
    "print('Took %.3f seconds to compute policy' % (time.time() - time1))\n",
    "\n",
    "print('### Computing Legible policy ###')\n",
    "time1 = time.time()\n",
    "leg_pol, leg_q = task_mdp_w.policy_iteration(goals.index(goal))\n",
    "print('Took %.3f seconds to compute policy' % (time.time() - time1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x0 = '1 1 N'\n",
    "goal_state = objs_states[goals.index(goal)]\n",
    "now = datetime.datetime.now()\n",
    "print('Initial State: ' + x0)\n",
    "\n",
    "# Get optimal trajectory\n",
    "print('Optimal trajectory for task: ' + goal)\n",
    "rng_gen = np.random.default_rng(int(time.time()))\n",
    "opt_states, opt_actions = opt_mdps['mdp' + str(goals.index(goal) + 1)].trajectory(x0, opt_pol, rng_gen)\n",
    "print('Trajectory: ' + str(opt_states))\n",
    "print('Cost: ' + str(opt_mdps['mdp' + str(goals.index(goal) + 1)].trajectory_reward([[opt_states, opt_actions]])))\n",
    "print('Legible Reward: ' + str(task_mdp_w.trajectory_reward([[opt_states, opt_actions]], goals.index(goal))))\n",
    "opt_traj = process_trajectory(opt_states, opt_actions)\n",
    "\n",
    "# Get legible trajectory\n",
    "print('Legible trajectory for task: ' + goal)\n",
    "rng_gen = np.random.default_rng(int(time.time()))\n",
    "leg_states, leg_act = task_mdp_w.trajectory(x0, leg_pol, rng_gen)\n",
    "print('Trajectory: ' + str(leg_states))\n",
    "print('Cost: ' + str(opt_mdps['mdp' + str(goals.index(goal) + 1)].trajectory_reward([[leg_states, leg_act]])))\n",
    "print('Legible Reward: ' + str(task_mdp_w.trajectory_reward([[leg_states, leg_act]], goals.index(goal))))\n",
    "leg_traj = process_trajectory(leg_states, leg_act)\n",
    "\n",
    "# Visualize both trajectories side-by-side\n",
    "figure = create_world_view(n_rows, n_cols, objs_states, walls)\n",
    "visualize_trajectory(opt_traj[0], opt_traj, figure, 'b', goal_state, zorder=0)\n",
    "visualize_trajectory(leg_traj[0], leg_traj, figure, 'k', goal_state, zorder=1)\n",
    "fig, _ = figure\n",
    "fig.show()\n",
    "\n",
    "# Simulate model performance\n",
    "print('Getting model performance!!')\n",
    "rng_gen = np.random.default_rng(int(time.time()))\n",
    "clock_1 = time.time()\n",
    "mdp_r, mdp_rl, leg_mdp_r, leg_mdp_rl = simulate(opt_mdps['mdp' + str(goals.index(goal) + 1)], opt_pol,\n",
    "                                                leg_mdp, leg_pol, x0, 1000, goals.index(goal), rng_gen)\n",
    "time_simulation = time.time() - clock_1\n",
    "print('Simulation length = %.3f' % time_simulation)\n",
    "print('Optimal Policy performance:\\nCost: %.3f\\nLegible Reward: %.3f' % (mdp_r, mdp_rl))\n",
    "print('Legible Policy performance:\\nCost: %.3f\\nLegible Reward: %.3f' % (leg_mdp_r, leg_mdp_rl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colors = ['blue', 'darkred', 'green', 'black', 'orange', 'pink', 'yellow', 'magenta', \n",
    "          'brown', 'cyan', 'khaki', 'olivedrab', 'lightcoral']\n",
    "x0 = '1 1 N'\n",
    "goal = 'O'\n",
    "\n",
    "\n",
    "opt_mdp = opt_mdps['mdp' + str(goals.index(goal) + 1)]\n",
    "opt_pol, _ = opt_mdp.policy_iteration()\n",
    "leg_mdp = LegibleTaskMDP(X_w, A_w, P_w, 0.9, True, goal, objs_states, goals, beta, \n",
    "                         get_goal_states(X_w, goals[goals.index(goal)]), 1, 'leg_optimal', q_mdps=opt_q_mdps, \n",
    "                         v_mdps=opt_v_mdps, dists=dists)\n",
    "leg_pol, _ = leg_mdp.policy_iteration(goals.index(goal))\n",
    "\n",
    "# Visualize all possible optimal trajectories for given starting state\n",
    "trajs, a_trajs = opt_mdp.all_trajectories(x0, opt_pol, rng_gen)\n",
    "rng_gen = np.random.default_rng(int(time.time()))\n",
    "print('Optimal trajectories: %d' % len(trajs))\n",
    "i = 0\n",
    "fig = create_world_view(n_rows, n_cols, objs_states, walls)\n",
    "for j in range(len(trajs)):\n",
    "    traj = trajs[j]\n",
    "    a_traj = a_trajs[j]\n",
    "    p_traj = process_trajectory(traj, a_traj)\n",
    "    visualize_trajectory(p_traj[0], p_traj, fig, colors[min(i, len(colors) - 1)], goal_state, zorder=(len(trajs) - j))\n",
    "    i += 1\n",
    "fig, _ = fig\n",
    "fig.show()\n",
    "\n",
    "# Visualize all possible legible trajectories for given starting state\n",
    "rng_gen = np.random.default_rng(int(time.time()))\n",
    "leg_trajs, leg_a_trajs = leg_mdp.all_trajectories(x0, leg_pol, rng_gen)\n",
    "print('Legible trajectories: %d' % len(leg_trajs))\n",
    "i = 0\n",
    "fig = create_world_view(n_rows, n_cols, objs_states, walls)\n",
    "for j in range(len(leg_trajs)):\n",
    "    traj = leg_trajs[j]\n",
    "    a_traj = leg_a_trajs[j]\n",
    "    p_traj = process_trajectory(traj, a_traj)\n",
    "    visualize_trajectory(p_traj[0], p_traj, fig, colors[min(i, len(colors) - 1)], goal_state, zorder=(len(trajs) - j))\n",
    "    i += 1\n",
    "fig, _ = fig\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "states = X_w\n",
    "print('Goal: ' + goal)\n",
    "\n",
    "# Visualize optimal policy\n",
    "opt_figure = create_world_view(n_rows, n_cols, objs_states, walls)\n",
    "draw_policy_states(opt_figure, pol_w, goal_states, states, list(A_w), 'b', with_objs)\n",
    "fig, _ = opt_figure\n",
    "fig.show()\n",
    "\n",
    "# Visualize legible policy\n",
    "leg_figure = create_world_view(n_rows, n_cols, objs_states, walls)\n",
    "draw_policy_states(leg_figure, task_pol_w, goal_states, states, list(A_w), 'k', with_objs)\n",
    "fig, _ = leg_figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
